\section{Background}\label{sec:background}

\emph{Common case TCP packet processing can be accelerated when split
  from its uncommon code paths and offered as a separate service,
  executing on isolated processor cores.} To motivate this rationale,
we first discuss the tradeoffs made by existing software network stack
architectures and TCP hardware offload designs. We then quantify these
tradeoffs for the TCP stack used inside the Linux kernel.

\subsection{Network Stack Architecture}

Network stack architecture has a well-established history and various
points in the design space have been investigated. We cover the most
relevant designs here. As we will see, all designs split TCP packet
processing into different components to achieve a different tradeoff
among performance, security, and functionality. \taas builds on this
history to arrive at its own, unique point in the design space.

\paragraph{Monolithic, in-kernel.} The most popular TCP stack design
is monolithic and resides completely in the OS kernel. A monolithic
TCP stack fulfills all of its functionality in software, as a single
block of code. Built for extensibility, it follows a deeply modular
design approach with complex inter-module dependencies. Each module
implements a different part of the stack's feature set, interconnected
via queues, function call APIs, and software interrupts. The stack
itself is trusted and to protect it from untrusted application code, a
split is made between application-level and stack-level packet
processing at the system call interface, involving a processor
privilege mode switch and associated data copies for security. This is
the design of the Linux, BSD, and Windows TCP network stacks. The
complex nature of these stacks leads them to execute a large number of
instructions per packet, with a high code and data footprint
($\S$\ref{sec:linux_overheads}).

\paragraph{Kernel bypass.} To alleviate the protection overheads of
in-kernel stacks, such as kernel-crossings, software mutliplexing, and
copying, kernel bypass network stacks split the responsibilities of
TCP packet processing into a trusted control plane and a (typically)
untrusted data plane. The control plane deals with connection and
protection setup and executes in the kernel, while the data plane
deals with common-case packet processing on existing connections and
is (typically) linked directly into the application. To enforce
control plane policy on the data plane, these approaches leverage
hardware IO virtualization support \cite{peter:arrakis, mtcp}. In
addition, this approach allows us to tailor the stack to the needs of
the application, excluding unneeded features for higher efficiency
\cite{sandstorm}. The downside of this approach is that, beyond
coarse-grained rate limiting and firewalling, there is no control over
congestion control or low-level transport protocol
behavior. Applications are free to send packets in any fashion they
see fit, within their limit. This can interact badly with the data
center's congestion control policy.

\paragraph{Protected kernel bypass.} To alleviate this particular
problem of kernel bypass network stacks, IX~\cite{belay:ix} leverages
hardware CPU virtualization to insert an intermediate layer of
protection, running the network stack in guest kernel mode, while the
OS kernel executes in host kernel mode. This allows us to deploy
trusted network stacks, while allowing them to be tailored and
streamlined for each application. However, this approach re-introduces
many of the overheads of the kernel-based approach.

\paragraph{NIC offload.} Various TCP offload engines have been
proposed in the past~\cite{chelsio_toe}. These engines leverage
various splits of TCP packet processing responsibilities and
distribute them among software executing on a CPU and a dedicated
hardware engine executing on the NIC. The most popular is TCP chimney
offload~\cite{chimney}, which retains the control plane within the OS
kernel and executes the data plane on the NIC. By offloading work from
CPUs to NICs, these designs achieve high energy-efficiency and free
CPUs from packet processing work. Their downside is that they are slow
and difficult to evolve and to customize. Their market penetration has
been low for this reason.

\paragraph{Dedicated CPUs.} These approaches dedicate CPUs to execute
the entire TCP stack~\cite{onload, flexsc}. These stacks interact with
applications via message queues instead of system calls, allowing them
to alleviate the indirect overheads of these calls, such as cache
pollution and pipeline stalls, and to batch calls for better
efficiency. Barrelfish~\cite{barrelfish} subdivides the stack further,
executing the NIC device driver, stack, and application, all on their
own dedicated cores. These approaches attain high and stable
throughput via pipeline parallelism and performance isolation among
stack and application. However, even when dedicating a number of
processors to the TCP stack, executing the entire stack can be
inefficient, causing pipeline stalls and cache misses due to
complexity. \taas builds on these approaches, but goes one step
further. By subdividing the TCP stack data plane into common and
uncommon code paths, dedicating separate cores for each, and
revisiting efficient stack implementation on modern processors, we can
attain close to optimal CPU efficieny.

\subsection{TCP Stack Overheads}\label{sec:linux_overheads}

To demonstrate the inefficiencies of kernel TCP stacks, we quantify
the overheads of the Linux TCP stack and compare them to \taas. To do
so, we instrument both stacks using hardware performance counters,
running a simple key-value store server benchmark. Our benchmark
server serves 512 concurrent connections from several client machines
that saturate the server network bandwidth with small requests for a
small working set, half the size of the server's L3 cache (details of
our experimental setup in $\S$\ref{sec:eval}). We execute the
experiment for two minutes and measure for one minute after warming up
for 30 seconds.

\begin{table}[t]
\centering
\begin{tabular}{l@{\hskip 4ex}rr@{\hskip 4ex}rr}
  % \toprule
  & \multicolumn{2}{c}{\textbf{Linux}} & \multicolumn{2}{c}{\textbf{TAS}}\\
  Module & kc & \% & kc & \%\\
  \midrule
  Driver    &  1.82 &   9\% & 0.21 &  3\% \\
  IP        &  2.63 &  13\% &    0 &  0\% \\
  TCP       &  5.32 &  26\% & 1.63 & 29\% \\
  Sockets   &  7.79 &  39\% & 2.69 & 48\% \\
  Other     &  1.04 &   5\% & 0.15 &  2\% \\
  App       &  1.53 &   8\% & 0.91 & 16\% \\
  \midrule
  Total     & 20.14 & 100\% & 5.59 & 100\%\\
  %\bottomrule
\end{tabular}
\caption{CPU cycles per request by network stack module.}
\label{tab:cycles_breakdown}
\end{table}

\paragraph{Linux overheads.} 
Table~\ref{tab:cycles_breakdown} shows the result. We find that Linux
executes 20.14 kilocycles (kc) for an average request, of which only
1.53 kc (7\%) are spent within the application, while the rest (93\%)
are spent within the Linux kernel. 87\% of total cycles are spent in
the network stack. For each request, Linux executes 12.5
kilo-instructions (ki), resulting in about 1.6 cycles per instruction
(CPI), 6.4$\times$ above the ideal 0.25 CPI for the server's 4-way
issue processor architecture. This results in high average per-request
processing latency of 9$\mu$s. The reason for these inefficiencies is
the computational complexity and high memory footprint of a
monolithic, in-kernel stack. Per-request privilege mode switches and
software interrupts stall processor pipelines; software multiplexing,
cross-module procedure calls, and security checks require additional
instructions; large, scattered per-connection state increases memory
footprint and causes stalls on cache and TLB misses; shared, scattered
global state causes stalls on locks and cache coherence, inflated by
coarse lock granularity and false sharing; covering all TCP packet
processing cases in one monolithic block causes the code to have many
branches, increasing instruction cache footprint and branch
mispredictions.

We measure these inefficiencies with CPU performance counters. The
results are shown in \autoref{tab:counters} and indicate the miss
events per request. Each request spends about 3.4 kc on L1 data cache
misses, incurs about 700 instruction cache misses, where an individual
miss on the critical path takes at least 12 cycles to resolve, and
also incurs 70 TLB misses at a cost of at least 14 cycles
each.%  \simon{We also might want to show where these misses occur. Do
  % the occurrences match with the above claims?}

% \paragraph{IX overheads.} For each request, IX executes 11 ki,
% resulting in about 0.5 CPI, still 2$\times$ above the ideal 0.25 CPI
% for the server. This results in an average per-request processing
% latency of 3$\mu$s. Privilege mode switches remain for IX and while IX
% simplifies the TCP stack, it still covers all TCP packet processing
% cases in one monolithic block, causing the code to have many branches,
% increasing instruction cache footprint and branch
% mispredictions. \autoref{tab:counters} shows that while IX reduces
% overheads across the board versus Linux, many branch mispredictions
% and instruction cache misses remain.

% The Linux network stacks has to execute a lot of instructions, and executes
% them inefficiently.
% \autoref{tab:cycles_breakdown} shows a more detailed breakdown of where these
% cycles are spent.
%\todo{double check this}.

\begin{table}[t]
\centering
\begin{tabular}{lrr}
  % \toprule
  Counter & Linux & \taas\\
  \midrule
  CPU cycles           & 1.5k/18.6k & 0.9k/4.7k\\
  Branch mispredicts   &       3/32 &      2/11\\
  I-cache misses       &     48/646 &       3/0\\
  DTLB misses          &       2/65 &       2/5\\
  Cycles stalled on L1 &  0.5k/2.9k &    300/1k\\
  % \bottomrule
\end{tabular}%
\caption{Per request user/stack overheads.}\label{tab:counters}%
\end{table}

\paragraph{\taas overheads.} \taas executes 6.5 ki per request,
resulting in 0.93 CPI, 3.7$\times$ above the ideal 0.25 CPI. While
\taas is not perfect, these cycles are executed on separate cores,
resulting in an average per-request processing latency of 2.6$\mu$s,
due to pipeline parallelism. \taas eliminates privilege mode switches
and branches by linearizing the fast path and executing it on an
isolated set of CPU cores. \autoref{tab:counters} shows that \taas
almost eliminates branch mispredictions and instruction cache misses.
