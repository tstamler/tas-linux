\section{Related Work}

% Data Center TCP (DCTCP), SIGCOMM 2010
% Misbehaving Receivers, CCR 1999
% Robust ECN Signalling with Nonces 
% https://www.cs.umd.edu/~nspring/talks/ietf-ecn.pdf
% \paragraph{Improvements to TCP.}
% The TCP protocol is constantly evolving, e.g., to improve TCP's attack
% resilience with nonces~\cite{misbehaving_receivers_ccr_1999}, to
% handle out of order packets more efficiently~\cite{juggler_eurosys16},
% to add better loss recovery mechanisms~\cite{aggression}, to improve
% congestion
% control~\cite{timely,dctcp,bbr,pcc,tcp_illinois,recursive_congestion,tcp_ex_machina,dcqcn},
% or to perform bandwidth allocation~\cite{numfabric,fastpass}. The key
% to adoption has been to keep the TCP packet format untouched. Our work
% is intended to be compatible with this line of work; \softtcp avoids
% binding protocol decisions into hardware.

\paragraph{Software TCP stack improvements.}
A closely related line of work aims to reduce TCP CPU overhead,
often with some level of NIC assistance; we push this farther by 
co-designing the NIC and software layers.
Many of these systems also use batching to reduce overhead at some cost in
latency; our focus is on reducing overhead for latency-sensitive RPCs 
where batching is less appropriate. 
Affinity-accept~\cite{affinity-accept} and Fastsocket~\cite{fastsocket} 
use flow steering on the NIC to keep connections local to cores. 
mTCP~\cite{mtcp} uses NIC virtualization to move the TCP stack
into each application, eliminating kernel calls in the common case,  
at the cost of trusting the application to implement congestion control.
Sandstorm~\cite{sandstorm} co-designs the TCP stack using 
application-specific knowledge about packet payloads; this is an interesting 
avenue for future work.
Megapipe~\cite{megapipe} re-designs the kernel-application interface 
around communication channels; we use a similar idea in our design. 
IX~\cite{belay:ix} pushes this farther by also changing the socket interface;
we aim to keep compatibility with existing applications.  To
improve load balancing, ZygOS~\cite{zygos} introduces an object steering 
layer that is similar to ours. Finally, CCP proposes separating congestion
control policy from its enforcement~\cite{ccp}; our work can be seen
as an implementation of that idea.

% Network stack redesigns:
% Scalable IP stacks using kernel bypass: mTCP (NSDI). Network stack specialization for performance (SIGCOMM'14).
% JUGGLER: A Practical Reordering Resilient Network Stack for
% Datacenters (EuroSys'16)
% Scalable TCP design for Linux (ASPLOS'16). affinity-accept. Megapipe.

Within a virtualized cloud context, NetKernel~\cite{netkernel} also
proposes to separate the network stack from guest OS kernels and to
offer it as a cloud service in a separate virtual machine. Unlike
\softtcp, NetKernel's goal is to allow cloud operators more control
over tenant network stacks and to accelerate provider-driven network
stack evolution by enabling new network protocol enhancements, such as
congestion control, and to make it available to all tenant VMs
transparently and simultaneously. \softtcp can provide the same
benefit, but our focus is on leveraging the separation of OS kernel
and network stack to streamline packet processing.

\paragraph{NIC-Software co-design.}
Previous attempts to improve packet processing performance used new
HW/SW interfaces to reduce the number of required PCIe transitions
\cite{flajslik:llnic,binkert:inic}, to scale rate
limiting~\cite{senic}, and to enable kernel-bypass
\cite{pratt:arsenic,voneicken:unet,druschel:osiris}. TCP Offload
Engines~\cite{toe,chelsio_toe} and remote direct memory access
(RDMA)~\cite{rdma} go a step further, entirely bypassing the remote
CPU for their specific use-case. Scale-out
NUMA~\cite{novakovic:sonuma} extends the RDMA approach by integrating
a remote memory access controller with the processor cache hierarchy
that automatically translates certain CPU memory accesses into remote
memory operations. Portals~\cite{portals_spec} is similar to RDMA, but
adds a set of offloadable memory and packet send operations triggered
upon matching packet arrival. Out of these approaches, only kernel
bypass has found broad market acceptance. One hindrance to widespread
adoption of network stack offload is that hardware stack deployment is
slower than software stack deployment, while application demands and
datacenter network deployments change rapidly. Hardware approaches are
thus often not able to keep pace fast enough with the changing world
around them. By providing an efficient software network stack,
\softtcp side-steps this issue, while providing performance close to
that of hardware solutions.

% An interesting future research direction
% is to show that the Portals model and \rmttcp can both be supported on
% the same configurable NIC hardware platform.
