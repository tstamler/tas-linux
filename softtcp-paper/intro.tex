\section{Introduction}

As network speeds rise, while CPU speeds stay stagnant, TCP packet
processing efficiency is becoming ever more important. Many data
center applications require low-latency and high-throughput access to
the network. At the same time, they rely on the lossless, in-order
delivery properties provided by TCP. To provide this convenience,
software TCP stacks consume an increasing fraction of CPU resources to
process network packets.

TCP processing overheads have been known for decades. In 1993, Van
Jacobson presented an implementation of TCP common-case receive
processing within 30 processor instructions~\cite{van_email}. Common
network stacks, such as Linux's, still use Van's performance
improvements~\cite{linux_van}. Still, a lot of CPU time goes into
packet processing and TCP stack processing latencies are high. For a
key-value store, Linux spends 9$\mu$s per request in TCP packet
processing. Even kernel-bypass network stacks, such as in
IX~\cite{belay:ix}, spend 3$\mu$s in per request TCP packet
processing. As network speeds continue to rise, these overheads
increasingly consume the available CPU time.

We investigate TCP packet processing overhead in the context of modern
processor and NIC hardware architecture. We find that existing TCP
stacks introduce overheads in various ways: 1. By running in
privileged mode on the same processor as the application, they induce
system call overhead and pollute the application-shared L1, L2, and
translation caches. 2. They spread per-connection state over several
cache lines, causing false sharing and reducing cache efficiency;
3. They share state over all processor cores in the machine, resulting
in cache coherence and locking overheads; 4. They execute the entire
TCP state machine to completion for each packet, resulting in code
with many branches that can be mis-predicted by CPUs and do not make
efficient use of batching and prefetching opportunities.
% ; 5. They do
% not make efficient use of modern NIC hardware facilities, such as hash
% calculations.

Instead, we harken back to TCP's origin as a computationally efficient
transport protocol, e.g., TCP congestion control was 
designed to avoid the use of integer multiplication and 
division~\cite{jacobsontcp}. 
Although TCP as a whole has become quite complex with many 
moving parts, the common case data path remains relatively simple. 
For example, packets sent within the data center are never fragmented 
at the IP layer, packets are almost always delivered reliably and in order.
and timeouts almost never fire.

We present TCP acceleration as a service (\softtcp), a lightweight
software TCP network stack offered as a separate service to the rest
of the system. We optimize for client-server RPCs using TCP as
transport, but we interoperate with legacy Linux TCP endpoints and can
support a variety of congestion control protocols including
TIMELY~\cite{timely} and DCTCP~\cite{dctcp}. Separating the TCP stack
from the OS kernel and offering it as a service enables a number of
streamlining opportunities: Like Van Jacobson, we realize that TCP
packet processing can be separated into a common and an uncommon
case. \softtcp is thus split into a lightweight stack (the \emph{fast
  path}) that handles common-case TCP packet processing and resource
enforcement, and a heavier stack (the \emph{slow path}) that processes
exceptions, such as connection setup/teardown, resource policy, and
timeouts. \softtcp executes the fast path on a set of dedicated CPUs,
holding the minimum TCP state necessary for common-case packet
processing in processor caches. Congestion control is implemented in
the slow path and enforced by the fast path, allowing precise control
over the allocation of network resources among competing flows by a
trusted control plane.  The fast path takes packets directly from (and
directly delivers packets to) user-level packet queues.  Unprivileged
application library code implements the POSIX socket abstraction on
top of these fast path queues.

We implement \softtcp as a user-level service intended to accelerate
the Linux OS kernel TCP stack. We evaluate \taas on a small cluster of
servers using microbenchmarks and common cloud application
workloads. Finally, we evaluate \taas' congestion control performance
at scale using simulations.
%
% \softtcp demonstrates 2$\times$ higher throughput and 3$\times$ lower
% latency than the Linux TCP stack. \softtcp provides 5$\times$ lower
% latency than Linux when the system is not operating at capacity by
% consolidating TCP processing on a smaller number of processors than
% Linux.
%
We make the following contributions:

\begin{compactitem}[\labelitemi]
\item We analyze the overheads of state-of-the-art TCP stacks in Linux
  and IX, demonstrating where they make inefficient use of modern
  processor hardware.

\item We present the design and implementation of \softtcp, a
  low-latency, low-overhead TCP network stack. \softtcp is fully
  compatible with existing TCP peers.

\item We present an overhead breakdown of \softtcp, showing that we
  eliminate the problems with existing TCP stacks.

\item We evaluate \softtcp on a set of microbenchmarks and common data
  center server applications, such as a key-value store, a real-time
  analytics framework, and a graph processing system. \softtcp
  provides up to 65\% lower latency and 50\% better throughput
  compared to the state-of-the-art IX kernel bypass OS.
\end{compactitem}

In the rest of this paper, we first provide background on TCP stack
design and present the overhead analysis of the stacks in Linux and IX
(Section~\ref{sec:background}). We then introduce the design of
\softtcp (Section~\ref{sec:design}) and describe its implementation
(Section~\ref{sec:impl}). Finally, we present the evaluation of
\softtcp in Section~\ref{sec:eval}.
